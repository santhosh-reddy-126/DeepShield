# ğŸ¤ DeepShield â€“ Audio Deepfake Detection System

ğŸ” A robust system to detect synthetic (deepfake) audio using CNN & Logistic Regression models, with SHAP-based explainability for transparency.

---

## ğŸ§  Overview

With the rise of AI-generated voice cloning and misinformation, detecting deepfake audio has become critical. **DeepShield** is an advanced audio deepfake detection tool that leverages machine learning and explainable AI techniques to ensure robust and transparent analysis.

It is designed to:

ğŸ¯ **Classify** whether an uploaded audio file is **real or fake**  
ğŸ§© **Analyze audio in 10-second chunks** to catch even subtle deepfake segments â€” a single fake chunk flags the entire file  
ğŸ“œ **Provide human-readable explanations** using SHAP values, so users can understand why a decision was made


## âœ… Features

- ğŸ—£ï¸ **Audio Upload**  
  Supports `.wav` and `.mp3` files

- â±ï¸ **Chunked Analysis**  
  Splits long audios into 10-second chunks and flags the entire file if any segment is suspicious

- ğŸ§® **MFCC + Other Features**  
  Uses 40 MFCCs + Chroma STFT + Spectral Contrast + Tonnetz + ZCR + RMSE

- ğŸ¤– **Dual Model Prediction**  
  Combines predictions from CNN and Logistic Regression models

- ğŸ“Š **SHAP Explainability**  
  Explains prediction decisions using SHAP values

- ğŸ’¡ **Feature Meanings**  
  Each feature has a human-understandable description

- ğŸ¨ **Streamlit UI**  
  User-friendly interface with styled visuals

- ğŸ§¾ **Plain English Summary**  
  Explains why an audio was flagged in simple terms

- ğŸ“ˆ **Interactive Force Plot**  
  Visualizes SHAP impact per feature

- ğŸ“Š **Bar Chart**  
  Shows top N most influential features

---

## ğŸ› ï¸ Technologies Used

- **Python** â€“ Core language  
- **Streamlit** â€“ Frontend/UI framework  
- **TensorFlow/Keras** â€“ CNN model building and inference  
- **scikit-learn** â€“ Logistic Regression model and preprocessing  
- **librosa** â€“ Audio processing and feature extraction  
- **pydub** â€“ Audio splitting, trimming, normalization  
- **SHAP** â€“ Model interpretability  
- **joblib / numpy** â€“ Data handling and preprocessing  
- **FFmpeg** â€“ Audio format conversion support

---

## ğŸ“ Dataset Structure

This project uses a combination of:

- âœ… **Real human speech samples**  
- âŒ **Deepfake audio samples** generated using tools like **Voicebox**, **Tacotron**, and **StyleGAN**

ğŸ§ **Used Dataset:**  
We used a custom-curated dataset combining samples from publicly available voice cloning repositories and synthetic datasets.

ğŸ“¦ **Download Dataset**:  
[ğŸ”— https://www.kaggle.com/datasets/mohammedabdeldayem/the-fake-or-real-dataset] 

ğŸ§¹ All audio files are preprocessed into consistent 10-second chunks and stored as NumPy arrays (`.npy`) or `.wav` files, ready for training.

---


## ğŸ§ª How It Works

### ğŸ”„ Step-by-Step Pipeline

1. **Upload Audio File** (`.wav` or `.mp3`)
2. **Audio Preprocessing**  
   - Trim silence  
   - Normalize volume  
   - Set consistent sample rate  
   - Pad or truncate to 10 seconds  
3. **Feature Extraction**  
   - MFCCs, Chroma, ZCR, RMSE, Spectral Contrast, Tonnetz  
4. **Prediction**  
   - Runs both CNN and Logistic Regression models  
   - If any chunk is fake â†’ whole audio is flagged  
5. **Explainability**  
   - Uses SHAP to show which features/time frames were suspicious  
6. **User Feedback**  
   - Displays result + visual explanation + plain English summary

---

## ğŸ“‹ Example Output

ğŸ”’ The audio is FAKE

Why?

MFCC_5: Subtle timbral differences â†’ High value suggests synthetic voice pattern

MFCC_36: Inter-harmonic spacing variation â†’ High value suggests unnatural modulation

MFCC_17: Rapid spectral slope fluctuations â†’ Low value suggests natural speech

## ğŸš€ Installation

```bash
git clone https://github.com/yourusername/deepshield-audio-deepfake.git   
cd deepshield-audio-deepfake
pip install -r requirements.txt
```

## Running the app
```bash
streamlit run audio.py
```

### âš ï¸ Note: Make sure to set the path to your local `ffmpeg.exe` for `pydub` to function correctly. Example:
```python
 from pydub import AudioSegment  
 AudioSegment.converter = "C:/ffmpeg/bin/ffmpeg.exe"
```



## ğŸ“š Feature Dictionary

| Feature Group        | Count | Description                                         |
|----------------------|-------|-----------------------------------------------------|
| **MFCC**             | 40    | Loudness, Pitch, Formants, Harmonics               |
| **Chroma**           | 12    | Pitch class energy, prosody indicators             |
| **Spectral Contrast**| 7     | Peak vs valley energy across frequency bands       |
| **Tonnetz**          | 6     | Tonal structure and key clarity                    |
| **ZCR**              | 1     | Unvoiced or noisy segments                         |
| **RMSE**             | 1     | Short-term loudness patterns                       |

ğŸ” You can find the full mapping in the code under `mfcc_meanings`.

---

## ğŸ“· Screenshots

- ![âœ… Streamlit UI](Screenshots/img1.png)
- ![âœ… SHAP force plots](Screenshots/img2.png)
- ![âœ… Bar chart of feature contributions](Screenshots/img3.png)
- ![âœ… Final prediction output with plain English summary](Screenshots/img4.png)
---

## ğŸ“Œ Future Enhancements (Optional)

- ğŸ“¥ Add batch processing mode  
- ğŸ“¼ Add video deepfake detection (multi-modal)  
- ğŸ“„ Export report (PDF/HTML)  
- ğŸŒ Deploy on Streamlit Cloud or Hugging Face Spaces  

---

## ğŸ™Œ Acknowledgements

- **Librosa** â€“ Audio analysis library  
- **SHAP** â€“ For model explainability  
- **Streamlit** â€“ For rapid web UI development  
- **FFmpeg** â€“ For audio conversion  

---

## ğŸ“¬ Contact

If you have questions, want to contribute, or need help adapting this model:

- ğŸ“§ **Email**: santhoshbeeram19@gmail.com  

