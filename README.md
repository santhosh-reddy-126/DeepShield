# ğŸ¤ DeepShield â€“ Audio Deepfake Detection System

ğŸ” A robust system to detect synthetic (deepfake) audio using CNN & Logistic Regression models, with SHAP-based explainability for transparency.

---

## ğŸ§  Overview

With the rise of AI-generated voice cloning and misinformation, detecting deepfake audio has become critical. DeepShield is an audio deepfake detection tool that uses machine learning and explainable AI to:

- ğŸ¯ Classify whether an uploaded audio is real or fake  
- ğŸ§  Highlight which parts of the audio led to the decision  
- ğŸ“œ Provide human-readable explanations using SHAP values

---

## âœ… Features

- ğŸ—£ï¸ **Audio Upload**  
  Supports `.wav` and `.mp3` files

- â±ï¸ **Chunked Analysis**  
  Splits long audios into 10-second chunks and flags the entire file if any segment is suspicious

- ğŸ§® **MFCC + Other Features**  
  Uses 40 MFCCs + Chroma STFT + Spectral Contrast + Tonnetz + ZCR + RMSE

- ğŸ¤– **Dual Model Prediction**  
  Combines predictions from CNN and Logistic Regression models

- ğŸ“Š **SHAP Explainability**  
  Explains prediction decisions using SHAP values

- ğŸ’¡ **Feature Meanings**  
  Each feature has a human-understandable description

- ğŸ¨ **Streamlit UI**  
  User-friendly interface with styled visuals

- ğŸ§¾ **Plain English Summary**  
  Explains why an audio was flagged in simple terms

- ğŸ“ˆ **Interactive Force Plot**  
  Visualizes SHAP impact per feature

- ğŸ“Š **Bar Chart**  
  Shows top N most influential features

- ğŸ•°ï¸ **Time Frame Mapping**  
  Maps SHAP values back to real time segments

---

## ğŸ› ï¸ Technologies Used

- **Python** â€“ Core language  
- **Streamlit** â€“ Frontend/UI framework  
- **TensorFlow/Keras** â€“ CNN model building and inference  
- **scikit-learn** â€“ Logistic Regression model and preprocessing  
- **librosa** â€“ Audio processing and feature extraction  
- **pydub** â€“ Audio splitting, trimming, normalization  
- **SHAP** â€“ Model interpretability  
- **joblib / numpy** â€“ Data handling and preprocessing  
- **FFmpeg** â€“ Audio format conversion support

---

## ğŸ“ Dataset Structure

Your training data should include both:

- âœ… Real human speech samples  
- âŒ Deepfake audio samples (e.g., generated using tools like Voicebox, Tacotron, or StyleGAN)

Preprocessed into NumPy arrays or compatible formats for training.

---

## ğŸ§ª How It Works

### ğŸ”„ Step-by-Step Pipeline

1. **Upload Audio File** (`.wav` or `.mp3`)
2. **Audio Preprocessing**  
   - Trim silence  
   - Normalize volume  
   - Set consistent sample rate  
   - Pad or truncate to 10 seconds  
3. **Feature Extraction**  
   - MFCCs, Chroma, ZCR, RMSE, Spectral Contrast, Tonnetz  
4. **Prediction**  
   - Runs both CNN and Logistic Regression models  
   - If any chunk is fake â†’ whole audio is flagged  
5. **Explainability**  
   - Uses SHAP to show which features/time frames were suspicious  
6. **User Feedback**  
   - Displays result + visual explanation + plain English summary

---

## ğŸ“‹ Example Output

ğŸ”’ The audio is FAKE

Why?

MFCC_5: Subtle timbral differences â†’ High value suggests synthetic voice pattern

MFCC_36: Inter-harmonic spacing variation â†’ High value suggests unnatural modulation

MFCC_17: Rapid spectral slope fluctuations â†’ Low value suggests natural speech

## ğŸš€ Installation

```bash
git clone https://github.com/yourusername/deepshield-audio-deepfake.git   
cd deepshield-audio-deepfake
pip install -r requirements.txt
```

## Running the app
```bash
streamlit run app.py
```

###âš ï¸ Note: Make sure to set the path to your local `ffmpeg.exe` for `pydub` to function correctly. Example:
```python
 from pydub import AudioSegment  
 AudioSegment.converter = "C:/ffmpeg/bin/ffmpeg.exe"
```



## ğŸ“š Feature Dictionary

| Feature Group        | Count | Description                                         |
|----------------------|-------|-----------------------------------------------------|
| **MFCC**             | 40    | Loudness, Pitch, Formants, Harmonics               |
| **Chroma**           | 12    | Pitch class energy, prosody indicators             |
| **Spectral Contrast**| 7     | Peak vs valley energy across frequency bands       |
| **Tonnetz**          | 6     | Tonal structure and key clarity                    |
| **ZCR**              | 1     | Unvoiced or noisy segments                         |
| **RMSE**             | 1     | Short-term loudness patterns                       |

ğŸ” You can find the full mapping in the code under `mfcc_meanings`.

---

## ğŸ“· Screenshots

ğŸ“¸ Include screenshots of:

- âœ… Streamlit UI  
- âœ… SHAP force plots  
- âœ… Bar chart of feature contributions  
- âœ… Final prediction output  

---

## ğŸ“Œ Future Enhancements (Optional)

- ğŸ“¥ Add batch processing mode  
- ğŸ“¼ Add video deepfake detection (multi-modal)  
- ğŸ“„ Export report (PDF/HTML)  
- ğŸŒ Deploy on Streamlit Cloud or Hugging Face Spaces  

---

## ğŸ™Œ Acknowledgements

- **Librosa** â€“ Audio analysis library  
- **SHAP** â€“ For model explainability  
- **Streamlit** â€“ For rapid web UI development  
- **FFmpeg** â€“ For audio conversion  

---

## ğŸ“„ License

MIT License â€“ see `LICENSE` for details.

---

## ğŸ“¬ Contact

If you have questions, want to contribute, or need help adapting this model:

- ğŸ“§ **Email**: santhoshbeeram19@gmail.com  

