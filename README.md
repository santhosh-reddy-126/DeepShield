# ğŸ¤ DeepShield â€“ Audio Deepfake Detection System

ğŸ” A robust system to detect synthetic (deepfake) audio using CNN & Logistic Regression models, with SHAP-based explainability for transparency.

---

## ğŸ§  Overview

With the rise of AI-generated voice cloning and misinformation, detecting deepfake audio has become critical. DeepShield is an audio deepfake detection tool that uses machine learning and explainable AI to:

- ğŸ¯ Classify whether an uploaded audio is real or fake  
- ğŸ§  Highlight which parts of the audio led to the decision  
- ğŸ“œ Provide human-readable explanations using SHAP values

---

## âœ… Features

- ğŸ—£ï¸ **Audio Upload**  
  Supports `.wav` and `.mp3` files

- â±ï¸ **Chunked Analysis**  
  Splits long audios into 10-second chunks and flags the entire file if any segment is suspicious

- ğŸ§® **MFCC + Other Features**  
  Uses 40 MFCCs + Chroma STFT + Spectral Contrast + Tonnetz + ZCR + RMSE

- ğŸ¤– **Dual Model Prediction**  
  Combines predictions from CNN and Logistic Regression models

- ğŸ“Š **SHAP Explainability**  
  Explains prediction decisions using SHAP values

- ğŸ’¡ **Feature Meanings**  
  Each feature has a human-understandable description

- ğŸ¨ **Streamlit UI**  
  User-friendly interface with styled visuals

- ğŸ§¾ **Plain English Summary**  
  Explains why an audio was flagged in simple terms

- ğŸ“ˆ **Interactive Force Plot**  
  Visualizes SHAP impact per feature

- ğŸ“Š **Bar Chart**  
  Shows top N most influential features

- ğŸ•°ï¸ **Time Frame Mapping**  
  Maps SHAP values back to real time segments

---

## ğŸ› ï¸ Technologies Used

- **Python** â€“ Core language  
- **Streamlit** â€“ Frontend/UI framework  
- **TensorFlow/Keras** â€“ CNN model building and inference  
- **scikit-learn** â€“ Logistic Regression model and preprocessing  
- **librosa** â€“ Audio processing and feature extraction  
- **pydub** â€“ Audio splitting, trimming, normalization  
- **SHAP** â€“ Model interpretability  
- **joblib / numpy** â€“ Data handling and preprocessing  
- **FFmpeg** â€“ Audio format conversion support

---

## ğŸ“ Dataset Structure

Your training data should include both:

- âœ… Real human speech samples  
- âŒ Deepfake audio samples (e.g., generated using tools like Voicebox, Tacotron, or StyleGAN)

Preprocessed into NumPy arrays or compatible formats for training.

---

## ğŸ§ª How It Works

### ğŸ”„ Step-by-Step Pipeline

1. **Upload Audio File** (`.wav` or `.mp3`)
2. **Audio Preprocessing**  
   - Trim silence  
   - Normalize volume  
   - Set consistent sample rate  
   - Pad or truncate to 10 seconds  
3. **Feature Extraction**  
   - MFCCs, Chroma, ZCR, RMSE, Spectral Contrast, Tonnetz  
4. **Prediction**  
   - Runs both CNN and Logistic Regression models  
   - If any chunk is fake â†’ whole audio is flagged  
5. **Explainability**  
   - Uses SHAP to show which features/time frames were suspicious  
6. **User Feedback**  
   - Displays result + visual explanation + plain English summary

---

## ğŸ“‹ Example Output

ğŸ”’ The audio is FAKE

Why?

MFCC_5: Subtle timbral differences â†’ High value suggests synthetic voice pattern

MFCC_36: Inter-harmonic spacing variation â†’ High value suggests unnatural modulation

MFCC_17: Rapid spectral slope fluctuations â†’ Low value suggests natural speech
